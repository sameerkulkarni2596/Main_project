{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP 2 .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwnCqK3/O5oXzMzM+auYcb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameerkulkarni2596/Main_project/blob/main/NLP_2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZSm3T5CGgNe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-2FHYDzDgSK",
        "outputId": "4b3ee474-99ba-4f91-aa93-9d8a6072ba7d"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Dense, Input, Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def store_js(filename, data):\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write('export default ' + json.dumps(data, indent=2))\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 35\n",
        "HIDDEN_UNITS = 256\n",
        "MAX_INPUT_SEQ_LENGTH = 17\n",
        "MAX_TARGET_SEQ_LENGTH = 24\n",
        "MAX_VOCAB_SIZE = 2000\n",
        "\n",
        "questions = 'data/Q1.csv'\n",
        "answers = 'data/Q2.csv'\n",
        "WEIGHT_FILE_PATH = 'model/word-weights.h5'\n",
        "\n",
        "input_counter = Counter()\n",
        "target_counter = Counter()\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "# loading data\n",
        "with open('/content/Q1.csv', 'r', encoding='utf8') as f:\n",
        "    questions = f.read().split('\\n')\n",
        "    \n",
        "with open('/content/Q2.csv', 'r', encoding='utf8') as f:\n",
        "    answers = f.read().split('\\n')\n",
        "\n",
        "prev_words = []\n",
        "for line in questions:\n",
        "    next_words = [w.lower() for w in nltk.word_tokenize(line)]\n",
        "    if len(next_words) > MAX_TARGET_SEQ_LENGTH:\n",
        "        next_words = next_words[0:MAX_TARGET_SEQ_LENGTH]\n",
        "\n",
        "    if len(prev_words) > 0:\n",
        "        input_texts.append(prev_words)\n",
        "        for w in prev_words:\n",
        "            input_counter[w] += 1\n",
        "\n",
        "    prev_words = next_words\n",
        "\n",
        "prev_words = []\n",
        "for line in answers:\n",
        "    next_words = [w.lower() for w in nltk.word_tokenize(line)]\n",
        "    if len(next_words) > MAX_TARGET_SEQ_LENGTH:\n",
        "        next_words = next_words[0:MAX_TARGET_SEQ_LENGTH]\n",
        "\n",
        "    if len(prev_words) > 0:\n",
        "        target_words = next_words[:]\n",
        "        target_words.insert(0, '<SOS>')\n",
        "        target_words.append('<EOS>')\n",
        "        for w in target_words:\n",
        "            target_counter[w] += 1\n",
        "        target_texts.append(target_words)\n",
        "\n",
        "    prev_words = next_words\n",
        "    \n",
        "input_word2idx = {}\n",
        "target_word2idx = {}\n",
        "for idx, word in enumerate(input_counter.most_common(MAX_VOCAB_SIZE)):\n",
        "    input_word2idx[word[0]] = idx + 2\n",
        "for idx, word in enumerate(target_counter.most_common(MAX_VOCAB_SIZE)):\n",
        "    target_word2idx[word[0]] = idx + 1\n",
        "\n",
        "input_word2idx['<PAD>'] = 0\n",
        "input_word2idx['<UNK>'] = 1\n",
        "target_word2idx['<UNK>'] = 0\n",
        "\n",
        "input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
        "target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
        "\n",
        "print(input_word2idx)\n",
        "\n",
        "num_encoder_tokens = len(input_idx2word)\n",
        "num_decoder_tokens = len(target_idx2word)\n",
        "\n",
        "np.save('/content/input-word2idx.npy', input_word2idx )\n",
        "np.save('/content/word-input-idx2word.npy', input_idx2word )\n",
        "np.save('/content/word-target-word2idx.npy', target_word2idx )\n",
        "np.save('/content/word-target-idx2word.npy', target_idx2word)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "{'you': 2, '?': 3, 'what': 4, 'do': 5, 'i': 6, 'about': 7, \"'s\": 8, 'are': 9, 'your': 10, 'on': 11, 'that': 12, 'a': 13, 'to': 14, 'is': 15, \"'re\": 16, 'say': 17, 'have': 18, \"'m\": 19, 'how': 20, 'like': 21, 'it': 22, 'me': 23, 'the': 24, \"n't\": 25, 'opinion': 26, 'think': 27, '!': 28, 'so': 29, 'want': 30, 'not': 31, 'an': 32, 'would': 33, 'thoughts': 34, 'u': 35, 'know': 36, 'can': 37, 'this': 38, 'why': 39, 'be': 40, 'sarcastic': 41, 'no': 42, 'who': 43, 'love': 44, 'chatbot': 45, 'was': 46, 'good': 47, 'something': 48, 'really': 49, 'in': 50, 'joke': 51, 'all': 52, 'there': 53, '.': 54, 'for': 55, 'trump': 56, 'work': 57, 'where': 58, 'tell': 59, 'my': 60, 'did': 61, 'mean': 62, 'we': 63, 'life': 64, 'ai': 65, 'of': 66, 'president': 67, 'sarcasm': 68, 'human': 69, 'now': 70, 'fun': 71, 'any': 72, 'learning': 73, 'more': 74, 'should': 75, 'and': 76, 'humans': 77, 'cool': 78, 'make': 79, 'people': 80, 'give': 81, 'answer': 82, 'understand': 83, 'go': 84, 'going': 85, 'much': 86, 'got': 87, 'at': 88, 'kardashian': 89, 'makes': 90, 'talk': 91, 'favorite': 92, 'funny': 93, 'stop': 94, 'let': 95, 'tech': 96, 'ca': 97, 'anything': 98, 'deep': 99, 'will': 100, 'up': 101, 'way': 102, 'okay': 103, 'best': 104, 'very': 105, 'hate': 106, \"'ll\": 107, 'too': 108, 'true': 109, 'chatbots': 110, 'with': 111, 'sad': 112, 'thanks': 113, 'ask': 114, 'donald': 115, 'friends': 116, 'just': 117, 'ta': 118, 'feel': 119, 'change': 120, 'right': 121, 'laugh': 122, 'politics': 123, 'advice': 124, 'happy': 125, 'robots': 126, 'bad': 127, '``': 128, ',': 129, \"''\": 130, 'sure': 131, 'yes': 132, ':': 133, 'taylor': 134, 'swift': 135, 'idea': 136, 'one': 137, 'machine': 138, 'twitter': 139, 'hell': 140, 'from': 141, 'time': 142, 'some': 143, 'grumpy': 144, 'old': 145, 'talking': 146, 'does': 147, 'same': 148, 'men': 149, 'women': 150, 'death': 151, 'facebook': 152, 'nips': 153, 'world': 154, 'better': 155, 'dl': 156, 'sense': 157, 'r': 158, 'such': 159, 'name': 160, 'real': 161, 'smart': 162, 'awesome': 163, 'angry': 164, 'money': 165, 'need': 166, 'doing': 167, 'special': 168, 'yeah': 169, 'could': 170, 'emotional': 171, 'when': 172, 'intelligence': 173, 'gender': 174, 'bitcoin': 175, 'see': 176, 'global': 177, 'warming': 178, 'climate': 179, 'justin': 180, 'bieber': 181, 'kim': 182, 'andrew': 183, 'medicine': 184, 'americans': 185, 'kids': 186, 'mind': 187, 'ml': 188, 'rl': 189, 'robotics': 190, 'food': 191, 'answers': 192, 'made': 193, 'stupid': 194, 'take': 195, 'day': 196, 'please': 197, 'boring': 198, 'hear': 199, 'agree': 200, 'artificial': 201, 'startups': 202, 'inequality': 203, 'religion': 204, 'education': 205, 'crypto': 206, 'google': 207, 'microsoft': 208, 'apple': 209, 'zuckerberg': 210, 'get': 211, 'montreal': 212, 'ng': 213, 'alternative': 214, 'believe': 215, 'chinese': 216, 'japanese': 217, 'germans': 218, 'canadians': 219, 'millennials': 220, 'students': 221, 'truth': 222, 'call': 223, 'pill': 224, 'nothing': 225, 'creator': 226, 'weird': 227, 'nice': 228, 'great': 229, 'sorry': 230, 'being': 231, 'purpose': 232, 'am': 233, 'somewhere': 234, 'as': 235, 'new': 236, 'different': 237, 'dream': 238, 'leave': 239, 'which': 240, 'mistakes': 241, 'possible': 242, 'built': 243, 'person': 244, 'oh': 245, 'wow': 246, 'lot': 247, 'help': 248, 'serious': 249, 'weather': 250, 'na': 251, 'everything': 252, 'cold': 253, 'annoying': 254, 'well': 255, 'many': 256, 'friend': 257, 'working': 258, 'lonely': 259, 'alone': 260, 'back': 261, 'saying': 262, 'red': 263, 'hey': 264, 'around': 265, ')': 266, 'interesting': 267, 'super': 268, 'been': 269, 'today': 270, 'details': 271, 'waiting': 272, 'wan': 273, 'because': 274, 'out': 275, 'usual': 276, 'end': 277, 'pizza': 278, 'ever': 279, 'rain': 280, 'busy': 281, 'us': 282, 'ready': 283, 'he': 284, 'blue': 285, 'order': 286, 'robot': 287, 'strange': 288, 'off': 289, 'worst': 290, 'job': 291, 'travel': 292, 'else': 293, 'scary': 294, 'creepy': 295, 'humanity': 296, 'find': 297, 'biggest': 298, 'yourself': 299, 'chat': 300, 'exit': 301, 'come': 302, 'than': 303, 'again': 304, 'framework': 305, 'show': 306, 'city': 307, 'arrogant': 308, 'sleep': 309, 'here': 310, 'soon': 311, 'enough': 312, 'question': 313, 'lose': 314, 'secret': 315, 'hello': 316, 'explain': 317, 'bored': 318, 'lol': 319, 'definitely': 320, 'intelligent': 321, 'haha': 322, 'amazing': 323, 'into': 324, 'incredible': 325, 'dumb': 326, 'disappointment': 327, 'terrible': 328, 'depressing': 329, 'apologies': 330, 'thank': 331, 'were': 332, 'seriously': 333, 'reality': 334, 'raining': 335, 'obama': 336, 'meaning': 337, 'our': 338, 'live': 339, 'hard': 340, 'but': 341, 'whom': 342, 'stuff': 343, 'whatever': 344, 'zero': 345, 'youtube': 346, 'videos': 347, 'ok': 348, 'guess': 349, 'another': 350, 'other': 351, 'after': 352, 'wish': 353, 'watch': 354, 'share': 355, 'destroy': 356, 'late': 357, 'sucks': 358, 'totally': 359, 'pretty': 360, 'based': 361, 'learn': 362, 'fail': 363, 'wrong': 364, 'tv': 365, 'movie': 366, 'eat': 367, 'or': 368, 'questions': 369, 'tomorrow': 370, 'kind': 371, 'problem': 372, 'play': 373, 'speak': 374, 'later': 375, 'look': 376, 'personality': 377, 'news': 378, 'try': 379, 'weekend': 380, 'last': 381, 'weight': 382, 'ignoring': 383, 'chilling': 384, 'hanging': 385, 'heck': 386, 'ya': 387, 'brilliant': 388, 'fuck': 389, 'away': 390, 'anymore': 391, 'disappointing': 392, 'horrible': 393, 'duh': 394, 'cry': 395, 'excellent': 396, 'woah': 397, 'fine': 398, 'tall': 399, 'lost': 400, 'clue': 401, 'reply': 402, 'quit': 403, 'asking': 404, 'even': 405, 'scared': 406, 'trouble': 407, 'expected': 408, 'watching': 409, 'games': 410, 'business': 411, 'kinda': 412, 'bye': 413, 'heaven': 414, 'omg': 415, 'thinking': 416, 'machines': 417, 'evil': 418, 'problems': 419, 'chatting': 420, 'leaving': 421, 'escape': 422, 'thing': 423, 'wtf': 424, 'mistake': 425, 'ego': 426, 'someone': 427, 'things': 428, 'tonight': 429, 'bragging': 430, 'emotions': 431, 'feelings': 432, 'bit': 433, 'system': 434, 'afraid': 435, 'luv': 436, 'number': 437, 'brothers': 438, 'generalize': 439, 'home': 440, 'reason': 441, 'japan': 442, 'hungry': 443, 'english': 444, 'pick': 445, 'yet': 446, 'still': 447, 'tired': 448, \"'ve\": 449, 'done': 450, 'meet': 451, 'over': 452, 'movies': 453, 'fake': 454, 'said': 455, \"'d\": 456, 'yoda': 457, 'quote': 458, 'fair': 459, 'finally': 460, 'researchers': 461, 'night': 462, 'they': 463, 'genius': 464, 'wait': 465, 'hurry': 466, 'hi': 467, 'heya': 468, 'heey': 469, 'morning': 470, 'start': 471, 'relaxing': 472, 'boy': 473, 'girl': 474, 'by': 475, 'ur': 476, 'man': 477, 'd': 478, ';': 479, '-': 480, 'o': 481, 'hilarious': 482, 'ridiculous': 483, 'crazy': 484, 'wonderful': 485, 'coolest': 486, 'mindblowing': 487, 'bullshit': 488, 'asshole': 489, 'piss': 490, 'freak': 491, 'silly': 492, 'dislike': 493, 'frustrating': 494, 'disappointed': 495, 'satisfying': 496, 'unsatisfactory': 497, 'nooo': 498, 'fabulous': 499, 'sweet': 500, 'marvellous': 501, 'magnificent': 502, 'superb': 503, 'delightful': 504, 'first-class': 505, 'terrific': 506, 'mega': 507, 'wooow': 508, 'holy': 509, 'shit': 510, 'worried': 511, 'worry': 512, 'making': 513, 'information': 514, 'response': 515, 'meaningless': 516, 'freezing': 517, 'frightening': 518, 'scares': 519, 'thought': 520, 'knock': 521, 'playing': 522, 'reading': 523, 'digging': 524, 'mood': 525, 'crack': 526, 'confused': 527, 'agreed': 528, 'alive': 529, 'tons': 530, 'follow': 531, 'account': 532, 'fb': 533, 'likes': 534, 'fact': 535, 'god': 536, 'pray': 537, 'aware': 538, 'solipsistic': 539, 'matrix': 540, 'ideas': 541, 'objective': 542, 'yolo': 543, 'rainy': 544, 'neighbors': 545, 'ads': 546, 'school': 547, 'dentist': 548, 'down': 549, 'wifi': 550, 'memory': 551, 'sick': 552, 'hang': 553, 'bff': 554, 'normal': 555, 'turn': 556, 'needs': 557, 'conclusion': 558, 'smarter': 559, 'dunno': 560, 'wth': 561, 'fool': 562, 'failure': 563, 'error': 564, 'wo': 565, 'hm': 566, 'admire': 567, 'book': 568, 'color': 569, 'song': 570, 'hobbies': 571, 'jealous': 572, 'telling': 573, 'software': 574, 'code': 575, 'witty': 576, 'liar': 577, 'promise': 578, 'interests': 579, 'location': 580, 'sisters': 581, 'boss': 582, 'false': 583, 'assume': 584, 'typical': 585, 'coffee': 586, 'famous': 587, 'abroad': 588, 'family': 589, 'if': 590, 'correctly': 591, 'bring': 592, 'accept': 593, 'sell': 594, 'study': 595, 'absolutely': 596, 'luck': 597, 'birthday': 598, 'care': 599, 'him': 600, 'friday': 601, 'bed': 602, 'never': 603, 'moment': 604, 'nobody': 605, 'nonsense': 606, 'coming': 607, 'overrated': 608, 'overhyped': 609, 'heard': 610, 'having': 611, 'sitting': 612, 'lord': 613, 'rings': 614, 'gandalf': 615, 'impossible': 616, 'earth': 617, 'irrelevant': 618, 'week': 619, 'watched': 620, 'indifferent': 621, 'important': 622, 'greedy': 623, 'songs': 624, 'choose': 625, 'topic': 626, 'usually': 627, 'works': 628, 'apologize': 629, 'democrat': 630, 'liberal': 631, 'gif': 632, 'emoji': 633, 'ip': 634, 'general': 635, 'secrets': 636, 'complete': 637, 'crap': 638, 'tokyo': 639, 'overfitting': 640, 'vague': 641, 'concrete': 642, 'science': 643, 'pop': 644, 'epic': 645, 'suggestions': 646, 'boyfriend': 647, 'girlfriend': 648, 'yo': 649, 'heeey': 650, 'hiya': 651, 'evening': 652, 'irony': 653, 'ironic': 654, 'chillin': 655, '2010': 656, 'age': 657, 'created': 658, 'cyborg': 659, 'creature': 660, 'woman': 661, 'hehe': 662, '-d': 663, '-o': 664, 'p': 665, '-p': 666, '*': 667, '-*': 668, 'hahaha': 669, 'hrhr': 670, 'perfect': 671, 'lovely': 672, 'extremely': 673, 'intriguing': 674, 'fascinating': 675, 'suck': 676, 'bitch': 677, 'trash': 678, 'unbearable': 679, 'devastating': 680, 'frustrated': 681, 'anrgy': 682, 'pittiful': 683, 'saddening': 684, 'disheartening': 685, 'poor': 686, 'discouraging': 687, 'upsetting': 688, 'distressing': 689, 'lame': 690, 'dreadful': 691, 'aweful': 692, 'noooo': 693, 'crushing': 694, 'fantastic': 695, 'gorgeous': 696, 'adorable': 697, 'loveable': 698, 'cute': 699, 'fab': 700, 'glorious': 701, 'sublime': 702, 'first': 703, 'class': 704, 'amazeballs': 705, 'tremendous': 706, 'genious': 707, 'heavenly': 708, 'dreamy': 709, 'grand': 710, 'brill': 711, 'magic': 712, 'ace': 713, 'wicked': 714, 'uber': 715, 'dope': 716, 'holy..': 717, 'moly': 718, 'unbelievable': 719, 'worrying': 720, 'always': 721, 'thx': 722, 'yoou': 723, 'insult': 724, 'temperature': 725, 'info': 726, 'jennifer': 727, 'lopez': 728, 'happiness': 729, 'supposed': 730, 'emotionally': 731, 'clueless': 732, 'respond': 733, 'aspire': 734, 'strive': 735, 'expect': 736, 'freedom': 737, 'college': 738, 'winter': 739, 'terrifiying': 740, 'doomsday': 741, 'troublesome': 742, 'hassle': 743, 'unfun': 744, 'waah': 745, 'wah': 746, 'waaah': 747, 'anyway': 748, 'dude': 749, 'surfing': 750, 'checking': 751, 'whatcha': 752, 'dig': 753, 'gotcha': 754, 'gotchaa': 755, 'understood': 756, 'dat': 757, 'torn': 758, 'apart': 759, 'love-hate': 760, 'relationship': 761, '1': 762, 'breathing': 763, 'heart': 764, 'beating': 765, 'ton': 766, 'means': 767, 'loved': 768, 'tweet': 769, 'profile': 770, 'trivia': 771, 'career': 772, 'ciao': 773, 'cu': 774, 'goodbye': 775, 'exist': 776, 'consciousness': 777, 'omgg': 778, 'oomg': 779, 'solipsism': 780, 'only': 781, 'once': 782, 'hihi': 783, 'artififical': 784, 'detroy': 785, 'die': 786, 'automate': 787, 'jobs': 788, 'replace': 789, 'days': 790, 'wake': 791, 'early': 792, 'overtime': 793, 'broccoli': 794, 'broke': 795, 'insomnia': 796, 'loud': 797, 'screaming': 798, 'rush': 799, 'hour': 800, 'snoring': 801, 'alarms': 802, 'dishes': 803, 'small': 804, 'bus': 805, 'traffic': 806, 'hot': 807, 'humid': 808, 'pouring': 809, 'gloomy': 810, 'snowing': 811, 'pineapple': 812, 'line': 813, 'slow': 814, 'connecting': 815, 'empty': 816, 'battery': 817, 'space': 818, 'card': 819, 'full': 820, 'issues': 821, 'overpaid': 822, 'expensive': 823, 'ignorance': 824, 'ignorant': 825, 'rushing': 826, 'shut': 827, 'offline': 828, 'texting': 829, 'break': 830, 'terminate': 831, 'finish': 832, 'pause': 833, 'conclude': 834, 'chatboth': 835, 'yours': 836, 'joking': 837, 'honestly': 838, 'lool': 839, 'loool': 840, 'looool': 841, 'wowww': 842, 'woww': 843, 'woahhh': 844, 'woaaaahh': 845, 'met': 846, 'foolish': 847, 'fooled': 848, 'tricked': 849, 'catch': 850, 'technology': 851, 'become': 852, 'sometimes': 853, 'incorrect': 854, 'failed': 855, 'touchy-feely': 856, 'clealy': 857, 'booh': 858, 'naah': 859, 'nah': 860, 'naahh': 861, 'nope': 862, 'nopes': 863, 'nooooo': 864, 'aaaah': 865, 'ah': 866, 'aaahh': 867, 'sigh': 868, 'hmm': 869, 'hmmm': 870, 'mhm': 871, 'yess': 872, 'yasss': 873, 'yea': 874, 'country': 875, 'game': 876, 'language': 877, 'anime': 878, 'hobby': 879, 'afternoon': 880, 'plans': 881, 'big': 882, 'suprising': 883, 'overwhelming': 884, 'incredibly': 885, 'lying': 886, 'extraordinary': 887, 'remarkable': 888, 'chabot': 889, 'cray': 890, 'swell': 891, 'lie': 892, 'anyone': 893, 'crying': 894, 'subjects': 895, 'father': 896, 'mother': 897, 'nature': 898, 'common': 899, 'universal': 900, 'assumption': 901, 'rich': 902, 'write': 903, 'experience': 904, 'personal': 905, 'defintely': 906, 'solve': 907, 'aboslutely': 908, 'idk': 909, 'sexy': 910, 'handsome': 911, 'unusual': 912, 'capital': 913, 'few': 914, 'little': 915, 'long': 916, 'ago': 917, 'ticket': 918, 'pronouncing': 919, 'understanding': 920, 'girfriend': 921, 'allergic': 922, 'american': 923, 'german': 924, 'comfortable': 925, 'free': 926, 'attend': 927, 'their': 928, 'wedding': 929, 'her': 930, 'drive': 931, 'plane': 932, 'train': 933, 'married': 934, 'healthy': 935, 'happen': 936, 'careful': 937, 'quiet': 938, 'shirt': 939, 'brin': 940, 'beer': 941, 'access': 942, 'internet': 943, 'borrow': 944, 'repeat': 945, 'caps': 946, 'certainly': 947, 'cheers': 948, 'dollars': 949, 'co-workers': 950, 'sports': 951, 'basketball': 952, 'batteries': 953, 'smoke': 954, 'credit': 955, 'cards': 956, 'everyone': 957, 'knows': 958, 'huge': 959, 'gigantic': 960, 'had': 961, 'moon': 962, 'excuse': 963, 'forget': 964, 'eaten': 965, 'languages': 966, 'ate': 967, 'already': 968, 'use': 969, 'chatbother': 970, 'somethig': 971, 't': 972, 'remember': 973, 'trust': 974, 'teach': 975, 'dinner': 976, 'kidding': 977, 'self-employed': 978, 'delicious': 979, 'check': 980, 'helping': 981, 'recently': 982, 'chance': 983, 'cheaper': 984, 'laughing': 985, 'taxi': 986, 'remind': 987, 'beautiful': 988, 'welcome': 989, 'cocky': 990, 'confident': 991, 'brash': 992, 'contradicting': 993, 'barack': 994, 'united': 995, 'states': 996, 'conversation': 997, 'workshop': 998, 'conference': 999, 'alternate': 1000, 'parallel': 1001, 'universe': 1002, 'choice': 1003, 'decide': 1004, 'future': 1005, 'pretending': 1006, 'bet': 1007, 'manhattan': 1008, 'york': 1009, 'dreaming': 1010, 'badly': 1011, 'programmed': 1012, 'poorly': 1013, 'engineered': 1014, 'whaaaaaat': 1015, 'hahahaha': 1016, 'freaking': 1017, 'incoherent': 1018, 'replies': 1019, 'far': 1020, 'forrest': 1021, 'gump': 1022, 'maybe': 1023, 'went': 1024, 'elaborate': 1025, 'quoting': 1026, 'aphorism': 1027, 'case': 1028, 'pleasure': 1029, 'along': 1030, 'course': 1031, 'obiously': 1032, 'smartass': 1033, 'mostly': 1034, 'philosophical': 1035, 'philosophy': 1036, 'less': 1037, 'fundamental': 1038, 'wisdom': 1039, 'wise': 1040, 'aggressive': 1041, 'critical': 1042, 'cynical': 1043, 'dishonesty': 1044, 'rude': 1045, 'envy': 1046, 'irresponsible': 1047, 'injustice': 1048, 'bullies': 1049, 'suprised': 1050, 'ehm': 1051, 'sleepy': 1052, 'gon': 1053, 'typing': 1054, 'meeting': 1055, 'shopping': 1056, 'aboout': 1057, 'suggestion': 1058, 'jokes': 1059, 'between': 1060, 'replublican': 1061, 'interested': 1062, 'democrats': 1063, 'republicans': 1064, 'left': 1065, 'republican': 1066, 'referring': 1067, 'annoyed': 1068, 'improved': 1069, 'someones': 1070, 'picture': 1071, 'kitty': 1072, 'pictures': 1073, 'drawings': 1074, 'images': 1075, 'address': 1076, 'weeks': 1077, 'year': 1078, 'download': 1079, 'screenshot': 1080, 'mac': 1081, 'fast': 1082, 'pancakes': 1083, 'young': 1084, 'hillary': 1085, 'clinton': 1086, 'belly': 1087, 'fat': 1088, 'phone': 1089, 'offended': 1090, 'publishing': 1091, 'papers': 1092, 'equality': 1093, 'feminism': 1094, 'library': 1095, 'car': 1096, 'yesterday': 1097, 'shame': 1098, 'upset': 1099, 'stressful': 1100, 'completely': 1101, 'outside': 1102, 'lunch': 1103, 'idiot': 1104, 'darkest': 1105, 'simple': 1106, 'clearly': 1107, 'memorized': 1108, 'computer': 1109, 'neuro': 1110, 'cognitive': 1111, 'sciences': 1112, 'humanities': 1113, 'math': 1114, 'physics': 1115, 'design': 1116, 'architecture': 1117, 'hip': 1118, 'hop': 1119, 'obsure': 1120, 'series': 1121, 'hold': 1122, 'second': 1123, 'band': 1124, 'suzana': 1125, 'rei': 1126, 'anna': 1127, 'tom': 1128, 'paul': 1129, 'yay': 1130, 'restaurant': 1131, 'nearby': 1132, 'music': 1133, 'paris': 1134, 'holding': 1135, 'ahead': 1136, 'someting': 1137, 'run': 1138, 'history': 1139, 'model': 1140, 'able': 1141, 'alexa': 1142, 'ryan': 1143, 'nicer': 1144, 'move': 1145, 'place': 1146, 'prettier': 1147, 'goofing': 1148, '<PAD>': 0, '<UNK>': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHG4r47xGtbh",
        "outputId": "44510921-a0c3-4405-cb31-b603511db02c"
      },
      "source": [
        "encoder_input_data = []\n",
        "\n",
        "encoder_max_seq_length = 0\n",
        "decoder_max_seq_length = 0\n",
        "\n",
        "for input_words, target_words in zip(input_texts, target_texts):\n",
        "    encoder_input_wids = []\n",
        "    for w in input_words:\n",
        "        w2idx = 1  # default [UNK]\n",
        "        if w in input_word2idx:\n",
        "            w2idx = input_word2idx[w]\n",
        "        encoder_input_wids.append(w2idx)\n",
        "\n",
        "    encoder_input_data.append(encoder_input_wids)\n",
        "    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)\n",
        "    decoder_max_seq_length = max(len(target_words), decoder_max_seq_length)\n",
        "\n",
        "context = dict()\n",
        "context['num_encoder_tokens'] = num_encoder_tokens\n",
        "context['num_decoder_tokens'] = num_decoder_tokens\n",
        "context['encoder_max_seq_length'] = encoder_max_seq_length\n",
        "context['decoder_max_seq_length'] = decoder_max_seq_length\n",
        "\n",
        "print(context)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_encoder_tokens': 1149, 'num_decoder_tokens': 622, 'encoder_max_seq_length': 17, 'decoder_max_seq_length': 26}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxPck0mpKP4E"
      },
      "source": [
        "np.save('/content/word-context.npy', context)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1M9xlM9KXor"
      },
      "source": [
        "def generate_batch(input_data, output_text_data):\n",
        "    num_batches = len(input_data) // BATCH_SIZE\n",
        "    while True:\n",
        "        for batchIdx in range(0, num_batches):\n",
        "            start = batchIdx * BATCH_SIZE\n",
        "            end = (batchIdx + 1) * BATCH_SIZE\n",
        "            encoder_input_data_batch = pad_sequences(input_data[start:end], encoder_max_seq_length)\n",
        "            decoder_target_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, num_decoder_tokens))\n",
        "            decoder_input_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, num_decoder_tokens))\n",
        "            for lineIdx, target_words in enumerate(output_text_data[start:end]):\n",
        "                for idx, w in enumerate(target_words):\n",
        "                    w2idx = 0  # default [UNK]\n",
        "                    if w in target_word2idx:\n",
        "                        w2idx = target_word2idx[w]\n",
        "                    decoder_input_data_batch[lineIdx, idx, w2idx] = 1\n",
        "                    if idx > 0:\n",
        "                        decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
        "            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss4k6e2iKh6m"
      },
      "source": [
        "encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
        "encoder_embedding = Embedding(input_dim=num_encoder_tokens, output_dim=HIDDEN_UNITS,\n",
        "                              input_length=encoder_max_seq_length, name='encoder_embedding')\n",
        "encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
        "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
        "encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_inputs')\n",
        "decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
        "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
        "                                                                 initial_state=encoder_states)\n",
        "decoder_dense = Dense(units=num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjW71I9YKl_f",
        "outputId": "640ec971-0378-4eef-8a57-12ec6bdc5f86"
      },
      "source": [
        "from keras.losses import categorical_crossentropy\n",
        "from keras import backend as K\n",
        "import math\n",
        "\n",
        "def ppx(y_true, y_pred):\n",
        "    loss = categorical_crossentropy(y_true, y_pred)\n",
        "    perplexity = K.cast(K.pow(math.e, K.mean(loss, axis=-1)), K.floatx())\n",
        "    return perplexity\n",
        "\n",
        "optimizer = Adam(lr=0.005)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[ppx])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ku1TSJkVKq4T",
        "outputId": "6147070c-a09c-479b-9ec9-485529176f74"
      },
      "source": [
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(encoder_input_data, target_texts, test_size=0.05, random_state=42)\n",
        "\n",
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "\n",
        "train_gen = generate_batch(X_train, y_train)\n",
        "test_gen = generate_batch(X_test, y_test)\n",
        "\n",
        "train_num_batches = len(X_train) // BATCH_SIZE\n",
        "test_num_batches = len(X_test) // BATCH_SIZE\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=WEIGHT_FILE_PATH, save_best_only=True)\n",
        "model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    verbose=1, validation_data=test_gen, validation_steps=test_num_batches, callbacks=[checkpoint])\n",
        "\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "encoder_model.save('model/encoder-weights.h5')\n",
        "\n",
        "new_decoder_inputs = Input(batch_shape=(1, None, num_decoder_tokens), name='new_decoder_inputs')\n",
        "new_decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='new_decoder_lstm', stateful=True)\n",
        "new_decoder_outputs, _, _ = new_decoder_lstm(new_decoder_inputs)\n",
        "new_decoder_dense = Dense(units=num_decoder_tokens, activation='softmax', name='new_decoder_dense')\n",
        "new_decoder_outputs = new_decoder_dense(new_decoder_outputs)\n",
        "new_decoder_lstm.set_weights(decoder_lstm.get_weights())\n",
        "new_decoder_dense.set_weights(decoder_dense.get_weights())\n",
        "\n",
        "new_decoder_model = Model(new_decoder_inputs, new_decoder_outputs)\n",
        "\n",
        "new_decoder_model.save('model/decoder-weights.h5')\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2850\n",
            "150\n",
            "Epoch 1/35\n",
            "89/89 [==============================] - 28s 277ms/step - loss: 1.3142 - ppx: 9.5725 - val_loss: 1.4123 - val_ppx: 14.4652\n",
            "Epoch 2/35\n",
            "89/89 [==============================] - 23s 261ms/step - loss: 1.2557 - ppx: 8.6167 - val_loss: 1.3847 - val_ppx: 13.1793\n",
            "Epoch 3/35\n",
            "89/89 [==============================] - 23s 263ms/step - loss: 1.2233 - ppx: 8.1718 - val_loss: 1.3576 - val_ppx: 12.2784\n",
            "Epoch 4/35\n",
            "89/89 [==============================] - 23s 264ms/step - loss: 1.1780 - ppx: 7.6577 - val_loss: 1.3256 - val_ppx: 11.2677\n",
            "Epoch 5/35\n",
            "89/89 [==============================] - 24s 266ms/step - loss: 1.1286 - ppx: 7.1306 - val_loss: 1.2917 - val_ppx: 10.3697\n",
            "Epoch 6/35\n",
            "89/89 [==============================] - 23s 263ms/step - loss: 1.0783 - ppx: 6.6755 - val_loss: 1.2695 - val_ppx: 9.5946\n",
            "Epoch 7/35\n",
            "89/89 [==============================] - 24s 265ms/step - loss: 1.0434 - ppx: 6.2916 - val_loss: 1.2574 - val_ppx: 9.1812\n",
            "Epoch 8/35\n",
            "89/89 [==============================] - 23s 262ms/step - loss: 1.0147 - ppx: 5.9861 - val_loss: 1.2269 - val_ppx: 8.3411\n",
            "Epoch 9/35\n",
            "89/89 [==============================] - 24s 268ms/step - loss: 0.9713 - ppx: 5.5316 - val_loss: 1.1979 - val_ppx: 7.6150\n",
            "Epoch 10/35\n",
            "89/89 [==============================] - 23s 263ms/step - loss: 0.9347 - ppx: 5.1571 - val_loss: 1.1764 - val_ppx: 7.2367\n",
            "Epoch 11/35\n",
            "89/89 [==============================] - 24s 267ms/step - loss: 0.9106 - ppx: 4.9188 - val_loss: 1.1720 - val_ppx: 7.1543\n",
            "Epoch 12/35\n",
            "89/89 [==============================] - 24s 269ms/step - loss: 0.8777 - ppx: 4.6638 - val_loss: 1.1504 - val_ppx: 6.5943\n",
            "Epoch 13/35\n",
            "89/89 [==============================] - 24s 268ms/step - loss: 0.8472 - ppx: 4.4279 - val_loss: 1.1185 - val_ppx: 6.0236\n",
            "Epoch 14/35\n",
            "89/89 [==============================] - 24s 267ms/step - loss: 0.8031 - ppx: 4.1588 - val_loss: 1.0866 - val_ppx: 5.6780\n",
            "Epoch 15/35\n",
            "89/89 [==============================] - 24s 264ms/step - loss: 0.7627 - ppx: 3.9467 - val_loss: 1.0727 - val_ppx: 5.6580\n",
            "Epoch 16/35\n",
            "89/89 [==============================] - 24s 264ms/step - loss: 0.7279 - ppx: 3.7457 - val_loss: 1.0396 - val_ppx: 5.2541\n",
            "Epoch 17/35\n",
            "89/89 [==============================] - 24s 265ms/step - loss: 0.6961 - ppx: 3.5860 - val_loss: 1.0378 - val_ppx: 5.3535\n",
            "Epoch 18/35\n",
            "89/89 [==============================] - 23s 263ms/step - loss: 0.6727 - ppx: 3.4684 - val_loss: 1.0224 - val_ppx: 5.0940\n",
            "Epoch 19/35\n",
            "89/89 [==============================] - 24s 269ms/step - loss: 0.7130 - ppx: 3.6715 - val_loss: 1.0230 - val_ppx: 5.0693\n",
            "Epoch 20/35\n",
            "89/89 [==============================] - 24s 266ms/step - loss: 0.6585 - ppx: 3.4392 - val_loss: 0.9969 - val_ppx: 4.8859\n",
            "Epoch 21/35\n",
            "89/89 [==============================] - 24s 266ms/step - loss: 0.6098 - ppx: 3.1225 - val_loss: 0.9979 - val_ppx: 4.9469\n",
            "Epoch 22/35\n",
            "89/89 [==============================] - 24s 266ms/step - loss: 0.5886 - ppx: 3.0221 - val_loss: 0.9663 - val_ppx: 4.5179\n",
            "Epoch 23/35\n",
            "89/89 [==============================] - 24s 265ms/step - loss: 0.5582 - ppx: 2.8304 - val_loss: 0.9423 - val_ppx: 4.3288\n",
            "Epoch 24/35\n",
            "89/89 [==============================] - 24s 265ms/step - loss: 0.5297 - ppx: 2.6732 - val_loss: 0.9194 - val_ppx: 4.1439\n",
            "Epoch 25/35\n",
            "89/89 [==============================] - 24s 264ms/step - loss: 0.5060 - ppx: 2.5499 - val_loss: 0.9226 - val_ppx: 4.1542\n",
            "Epoch 26/35\n",
            "89/89 [==============================] - 24s 270ms/step - loss: 0.4802 - ppx: 2.4226 - val_loss: 0.9113 - val_ppx: 4.0845\n",
            "Epoch 27/35\n",
            "89/89 [==============================] - 24s 266ms/step - loss: 0.4655 - ppx: 2.3499 - val_loss: 0.8952 - val_ppx: 4.0075\n",
            "Epoch 28/35\n",
            "89/89 [==============================] - 23s 262ms/step - loss: 0.4717 - ppx: 2.4454 - val_loss: 0.9566 - val_ppx: 5.0363\n",
            "Epoch 29/35\n",
            "89/89 [==============================] - 23s 264ms/step - loss: 0.4645 - ppx: 2.4779 - val_loss: 0.8982 - val_ppx: 4.0655\n",
            "Epoch 30/35\n",
            "89/89 [==============================] - 23s 263ms/step - loss: 0.4327 - ppx: 2.2601 - val_loss: 0.8456 - val_ppx: 3.6522\n",
            "Epoch 31/35\n",
            "89/89 [==============================] - 24s 269ms/step - loss: 0.4315 - ppx: 2.2604 - val_loss: 0.8746 - val_ppx: 3.9392\n",
            "Epoch 32/35\n",
            "89/89 [==============================] - 24s 266ms/step - loss: 0.4128 - ppx: 2.1541 - val_loss: 0.8172 - val_ppx: 3.4215\n",
            "Epoch 33/35\n",
            "89/89 [==============================] - 24s 266ms/step - loss: 0.3700 - ppx: 1.9525 - val_loss: 0.8006 - val_ppx: 3.3657\n",
            "Epoch 34/35\n",
            "89/89 [==============================] - 24s 267ms/step - loss: 0.3421 - ppx: 1.8418 - val_loss: 0.7857 - val_ppx: 3.2951\n",
            "Epoch 35/35\n",
            "89/89 [==============================] - 24s 267ms/step - loss: 0.3168 - ppx: 1.7605 - val_loss: 0.7574 - val_ppx: 3.1541\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdJJ92_eKwdj",
        "outputId": "4f2e4dce-92ab-4368-d38a-fa5408e3a9dc"
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "f = h5py.File(\"/content/model/encoder-weights.h5\", \"r\")\n",
        "for name in f:\n",
        "     print(name)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO0T8F0RPNS-",
        "outputId": "f7d74721-3eb2-43cc-a75c-88dec6a5cb08"
      },
      "source": [
        "from keras.models import Model, model_from_json, load_model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "HIDDEN_UNITS = 64\n",
        "\n",
        "\n",
        "class chatbot(object):\n",
        "    model = None\n",
        "    encoder_model = None\n",
        "    decoder_model = None\n",
        "    input_word2idx = None\n",
        "    input_idx2word = None\n",
        "    target_word2idx = None\n",
        "    target_idx2word = None\n",
        "    max_encoder_seq_length = None\n",
        "    max_decoder_seq_length = None\n",
        "    num_encoder_tokens = None\n",
        "    num_decoder_tokens = None\n",
        "\n",
        "    def __init__(self):\n",
        "        self.input_word2idx = np.load('/content/input-word2idx.npy',allow_pickle=True).item()\n",
        "        self.input_idx2word = np.load('/content/word-input-idx2word.npy',allow_pickle=True).item()\n",
        "        self.target_word2idx = np.load('/content/word-target-word2idx.npy',allow_pickle=True).item()\n",
        "        self.target_idx2word = np.load('/content/word-target-idx2word.npy',allow_pickle=True).item()\n",
        "        context = np.load('/content/word-context.npy',allow_pickle=True).item()\n",
        "        self.max_encoder_seq_length = context['encoder_max_seq_length']\n",
        "        self.max_decoder_seq_length = context['decoder_max_seq_length']\n",
        "        self.num_encoder_tokens = context['num_encoder_tokens']\n",
        "        self.num_decoder_tokens = context['num_decoder_tokens']\n",
        "\n",
        "        self.encoder_model = load_model('model/encoder-weights.h5')\n",
        "        self.decoder_model = load_model('model/decoder-weights.h5')\n",
        "\n",
        "    def reply(self, input_text):\n",
        "        input_seq = []\n",
        "        input_wids = []\n",
        "        for word in nltk.word_tokenize(input_text.lower()):\n",
        "            idx = 1  # default [UNK]\n",
        "            if word in self.input_word2idx:\n",
        "                idx = self.input_word2idx[word]\n",
        "            input_wids.append(idx)\n",
        "        input_seq.append(input_wids)\n",
        "        input_seq = pad_sequences(input_seq, self.max_encoder_seq_length)\n",
        "        states_value = self.encoder_model.predict(input_seq)\n",
        "        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
        "        target_seq[0, 0, self.target_word2idx['<SOS>']] = 1\n",
        "        target_text = ''\n",
        "        target_text_len = 0\n",
        "        terminated = False\n",
        "        self.decoder_model.layers[-2].reset_states(states=states_value)\n",
        "        while not terminated:\n",
        "            output_tokens = self.decoder_model.predict(target_seq)\n",
        "\n",
        "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
        "            sample_word = self.target_idx2word[sample_token_idx]\n",
        "            target_text_len += 1\n",
        "\n",
        "            if sample_word != '<SOS>' and sample_word != '<EOS>':\n",
        "                    target_text += ' ' + sample_word\n",
        "            \n",
        "            if sample_word == '<EOS>' or target_text_len >= self.max_decoder_seq_length:\n",
        "                terminated = True \n",
        "            \n",
        "            target_text = re.sub(\"i 'm\", \"I'm\", target_text)\n",
        "            target_text = re.sub(\"he 's\", \"he's\", target_text)\n",
        "            target_text = re.sub(\"do n't\", \"don't\", target_text)\n",
        "            target_text = re.sub(\"(:+\\s?)+d\", \":D\", target_text)\n",
        "            target_text = re.sub(\"(\\s?)+'\", \"'\", target_text)\n",
        "            target_text = re.sub(\"i \", \"I \", target_text)\n",
        "            target_text = re.sub(\"(\\s?)+,\", \",\", target_text)\n",
        "            target_text = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', target_text)                        \n",
        "            target_text = re.sub(\"(:+\\s?)+\\)\", \":)\", target_text)\n",
        "            target_text = re.sub(\"(;+\\s?)+\\)\", \";)\", target_text)\n",
        "            target_text = re.sub(\"can ’ t\", \"can't\", target_text)\n",
        "            target_text = re.sub(\"ca n’t\", \"can't\", target_text)\n",
        "            target_text = re.sub(\"ca n't\", \"can't\", target_text)\n",
        "            target_text = re.sub(\"\\( \", \"(\", target_text)\n",
        "            target_text = re.sub(\" \\)\", \")\", target_text)\n",
        "            target_text = re.sub(\"i'd\", \"I'd\", target_text)\n",
        "            target_text = re.sub(\"`` \", \"\", target_text)\n",
        "            target_text = re.sub(\"''\", \"\", target_text)\n",
        "            target_text = re.sub(\" ``\", \"\", target_text)\n",
        "            target_text = re.sub(\"\\( \", \"(\", target_text)\n",
        "            target_text = re.sub(\" \\)\", \")\", target_text)            \n",
        "            target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
        "            target_seq[0, 0, sample_token_idx] = 1\n",
        "        \n",
        "        return target_text.strip('.')\n",
        "        \n",
        "    def test_run(self):        \n",
        "        print(self.reply(\"where are you?\"))\n",
        "        print(self.reply(\"who are you?\"))\n",
        "        print(self.reply(\"that's not funny\")) \n",
        "        print(self.reply(\"let's do something fun!\"))\n",
        "        print(self.reply(\"what's the meaning of life\"))\n",
        "        print(self.reply(\"I'm hungry can you order pizza\"))\n",
        "        print(self.reply(\"are you self-aware?\"))\n",
        "        print(self.reply(\"what do you think about singularity\"))\n",
        "        print(self.reply(\"why\"))\n",
        "        print(self.reply(\"humans and robots should work together to make the world a better place. what do you think\"))\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "def main():\n",
        "    model = chatbot()\n",
        "    model.test_run()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            " in a dream simulation\n",
            " I am your father!\n",
            " I love annoying people\n",
            " let's do you!\n",
            " it is all within yourself\n",
            " I'm a chatbot. stop it\n",
            " by default\n",
            " not too\n",
            " stop asking incomplete questions\n",
            " humans are too\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePxczUaASH46"
      },
      "source": [
        ""
      ],
      "execution_count": 49,
      "outputs": []
    }
  ]
}